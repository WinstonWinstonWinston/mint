{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01a124f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from mint.state import MINTState\n",
    "from mint.data.ADP.ADP_dataset import ADPDataset\n",
    "from mint.module import MINTModule\n",
    "from mint.experiment.train import Train\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import logging\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1125d97-ae51-4624-be7c-dabef128573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:: No processed data found at /users/1/sull1276/mint/tests/../mint/data/ADP/AA_train.pkl.zst... preprocessing data\n",
      "INFO:: No processed data found at /users/1/sull1276/mint/tests/../mint/data/ADP/AA_test.pkl.zst... preprocessing data\n",
      "INFO:: No processed data found at /users/1/sull1276/mint/tests/../mint/data/ADP/AA_valid.pkl.zst... preprocessing data\n"
     ]
    }
   ],
   "source": [
    "ds_train = ADPDataset(data_dir='/users/1/sull1276/mint/tests/../mint/data/ADP', \n",
    "                       data_proc_fname=\"AA\", \n",
    "                       data_proc_ext=\".pkl.zst\", \n",
    "                       data_raw_fname=\"alanine-dipeptide-250ns-nowater\", \n",
    "                       data_raw_ext=\".xtc\", \n",
    "                       split=\"train\", \n",
    "                       total_frames_train=25000, \n",
    "                       total_frames_test=5000, \n",
    "                       total_frames_valid=5000, \n",
    "                       lag= OmegaConf.create({\"equilibrium\": True}), \n",
    "                       normalize= OmegaConf.create({\"bool\": False, \"t_dependent\": False}), \n",
    "                       node_features= OmegaConf.create({\"epsilon\": True, \"sigma\": True, \"charge\": True, \"mass\": True}), \n",
    "                       augement_rotations=False)\n",
    "\n",
    "ds_test = ADPDataset(data_dir='/users/1/sull1276/mint/tests/../mint/data/ADP', \n",
    "                       data_proc_fname=\"AA\", \n",
    "                       data_proc_ext=\".pkl.zst\", \n",
    "                       data_raw_fname=\"alanine-dipeptide-250ns-nowater\", \n",
    "                       data_raw_ext=\".xtc\", \n",
    "                       split=\"test\", \n",
    "                       total_frames_train=25000, \n",
    "                       total_frames_test=5000, \n",
    "                       total_frames_valid=5000, \n",
    "                       lag= OmegaConf.create({\"equilibrium\": True}), \n",
    "                       normalize= OmegaConf.create({\"bool\": False, \"t_dependent\": False}), \n",
    "                       node_features= OmegaConf.create({\"epsilon\": True, \"sigma\": True, \"charge\": True, \"mass\": True}), \n",
    "                       augement_rotations=False)\n",
    "\n",
    "ds_valid = ADPDataset(data_dir='/users/1/sull1276/mint/tests/../mint/data/ADP', \n",
    "                       data_proc_fname=\"AA\", \n",
    "                       data_proc_ext=\".pkl.zst\", \n",
    "                       data_raw_fname=\"alanine-dipeptide-250ns-nowater\", \n",
    "                       data_raw_ext=\".xtc\", \n",
    "                       split=\"valid\", \n",
    "                       total_frames_train=25000, \n",
    "                       total_frames_test=5000, \n",
    "                       total_frames_valid=5000, \n",
    "                       lag= OmegaConf.create({\"equilibrium\": True}), \n",
    "                       normalize= OmegaConf.create({\"bool\": False, \"t_dependent\": False}), \n",
    "                       node_features= OmegaConf.create({\"epsilon\": True, \"sigma\": True, \"charge\": True, \"mass\": True}), \n",
    "                       augement_rotations=False)\n",
    "module = MINTModule(\n",
    "    cfg=OmegaConf.create({\n",
    "        \"prior\": {\n",
    "            \"_target_\": \"mint.prior.normal.NormalPrior\",\n",
    "            \"mean\": 0.0,\n",
    "            \"std\": 0.25,\n",
    "        },\n",
    "        \"embedder\": {\n",
    "            \"_target_\": \"mint.model.embedding.equilibrium_embedder.EquilibriumEmbedder\",\n",
    "            \"use_ff\": True,\n",
    "            \"interp_time\": {\n",
    "                \"embedding_dim\": 64,\n",
    "                \"max_positions\": 1000,\n",
    "            },\n",
    "            \"force_field\": {\n",
    "                \"in_dim\": 4,\n",
    "                \"hidden_dims\": [128, 64],\n",
    "                \"out_dim\": 32,\n",
    "                \"activation\": \"relu\",\n",
    "                \"use_input_bn\": True,\n",
    "                \"affine\": True,\n",
    "                \"track_running_stats\": True,\n",
    "            },\n",
    "            \"atom_type\": {\n",
    "                \"num_types\": 14,\n",
    "                \"embedding_dim\": 32,\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"_target_\": \"mint.model.equivariant.transformer.MultiSE3Transformer\",\n",
    "            \"input_channels\": [[128], [0]],\n",
    "            \"readout_channels\": [[0, 0], [0, 1]],\n",
    "            \"hidden_channels\": [[8, 8], [8, 8]],\n",
    "            \"key_channels\": [[8, 8], [8, 8]],\n",
    "            \"query_channels\": [[8, 8], [8, 8]],\n",
    "            \"edge_l_max\": 2,\n",
    "            \"edge_basis\": \"smooth_finite\",\n",
    "            \"max_radius\": 10,\n",
    "            \"number_of_basis\": 64,\n",
    "            \"hidden_size\": 128,\n",
    "            \"max_neighbors\": 10000,\n",
    "            \"act\": \"silu\",\n",
    "            \"num_layers\": 4,\n",
    "            \"bn\": True,\n",
    "        },\n",
    "        \"interpolant\": {\n",
    "            \"_target_\": \"mint.interpolant.interpolants.TemporallyLinearInterpolant\",\n",
    "            \"velocity_weight\": 1.0,\n",
    "            \"denoiser_weight\": 1.0,\n",
    "            \"gamma_weight\": 0.1,\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"stratified\": False,\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"optimizer\": {\n",
    "                \"name\": \"Adam\",\n",
    "                \"lr\": 3e-4,\n",
    "                \"weight_decay\": 0.01,\n",
    "                \"betas\": [0.9, 0.999],\n",
    "            },\n",
    "            \"scheduler\": {\n",
    "                \"name\": \"CosineAnnealingLR\",\n",
    "                \"T_max\": \"experiment.train.trainer.max_epochs\",\n",
    "                \"eta_min\": 1e-6,\n",
    "            },\n",
    "        },\n",
    "    })\n",
    ")\n",
    "    \n",
    "st = MINTState(\n",
    "    seed=42,\n",
    "    module=module,\n",
    "    dataset_train=ds_train,\n",
    "    dataset_valid=ds_valid,\n",
    "    dataset_test=ds_test,\n",
    ")\n",
    "\n",
    "train_cfg = OmegaConf.create({\n",
    "    \"trainer\": {\n",
    "        \"overfit_batches\": 0,\n",
    "        \"min_epochs\": 1,\n",
    "        \"max_epochs\": 200,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"deterministic\": False,\n",
    "        # \"strategy\": \"ddp_notebook\",\n",
    "        \"val_check_interval\": 1.0,\n",
    "        \"check_val_every_n_epoch\": 1,\n",
    "        \"accumulate_grad_batches\": 1,\n",
    "        \"gradient_clip_val\": 0.5,\n",
    "        \"gradient_clip_algorithm\": \"norm\",\n",
    "        \"precision\": \"32-true\",\n",
    "    },\n",
    "    \"checkpointer\": {\n",
    "        \"dirpath\": \"/users/1/sull1276/mint/tests/logs/hydra/ckpt\",\n",
    "        \"save_last\": True,\n",
    "        \"save_top_k\": 5,\n",
    "        \"monitor\": \"val/loss\",\n",
    "        \"filename\": \"epoch_{epoch}-step_{step}-loss_{val/loss:.4f}\",\n",
    "        \"auto_insert_metric_name\": False,\n",
    "        \"mode\": \"min\",\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"name\": \"mint\",\n",
    "        \"project\": \"mint\",\n",
    "        \"save_dir\": \"/users/1/sull1276/mint/tests/logs/wandb\",\n",
    "    },\n",
    "    \"wandb_watch\": {\n",
    "        \"log\": \"all\",\n",
    "        \"log_freq\": 500,\n",
    "    },\n",
    "    \"warm_start\": None,\n",
    "    \"warm_start_cfg_override\": True,\n",
    "    \"loader\": {\n",
    "        \"num_workers\": 8,\n",
    "        \"prefetch_factor\": 2,\n",
    "        \"batch_size\": {\n",
    "            \"train\": 64,\n",
    "            \"valid\": 64,\n",
    "            \"test\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"num_device\": 1,\n",
    "    \"project\": {\"name\": \"mint\"}\n",
    "})\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n",
    "for level in logging_levels:\n",
    "    setattr(logger, level, rank_zero_only(getattr(logger, level)))\n",
    "    \n",
    "trainer = Train(st, train_cfg, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38724d5f-f611-4f25-8b73-f142b3d76176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwinsaton\u001b[0m (\u001b[33mwinsaton-univeristy-of-minnesota\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/1/sull1276/mint/tests/logs/wandb/wandb/run-20251112_171802-cwle8160</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/winsaton-univeristy-of-minnesota/mint/runs/cwle8160' target=\"_blank\">mint</a></strong> to <a href='https://wandb.ai/winsaton-univeristy-of-minnesota/mint' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/winsaton-univeristy-of-minnesota/mint' target=\"_blank\">https://wandb.ai/winsaton-univeristy-of-minnesota/mint</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/winsaton-univeristy-of-minnesota/mint/runs/cwle8160' target=\"_blank\">https://wandb.ai/winsaton-univeristy-of-minnesota/mint/runs/cwle8160</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /users/1/sull1276/micromamba/envs/e3ti/lib/python3.1 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /users/1/sull1276/mint/tests/logs/hydra/ckpt exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | embedder | EquilibriumEmbedder | 11.6 K | train\n",
      "1 | model    | MultiSE3Transformer | 855 K  | train\n",
      "---------------------------------------------------------\n",
      "866 K     Trainable params\n",
      "0         Non-trainable params\n",
      "866 K     Total params\n",
      "3.467     Total estimated model params size (MB)\n",
      "103       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdeb7a82f0345c2adb694e06cebbb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1408. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fade48c8d03f4e30bd2fd820cc5b19eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ebdb93e0845ff82e0e7994936b124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 176. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cd87f9df25401485019cb2e6e4cc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19b1011f3bf4b0785cf46e3e068a2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a09353-1e9c-47d5-af4d-d9c8f118068f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (e3ti)\n",
   "language": "python",
   "name": "e3ti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
